{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NaiveBayes import spark, process_dataframe, sc\n",
    "from math import log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"../../DataFiles/airline-train.csv\")\n",
    "\n",
    "df, string_indexers, bucketizers = process_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label used:  satisfaction_index\n",
      "Features used:  ['Leg room service', 'Checkin service', 'Seat comfort', 'Inflight entertainment', 'Flight Distance_bucket', 'Type of Travel_index', 'Departure/Arrival time convenient', 'Age_bucket', 'Online boarding', 'Gate location', 'Cleanliness', 'Ease of Online booking', 'Class_index', 'Inflight service', 'Inflight wifi service', 'Arrival Delay in Minutes_bucket', 'Baggage handling', 'Departure Delay in Minutes_bucket', 'Customer Type_index', 'On-board service', 'Gender_index', 'Food and drink']\n"
     ]
    }
   ],
   "source": [
    "LABEL_COL = \"satisfaction_index\"\n",
    "FEATURES_COL = list(set(df.columns) - set([LABEL_COL]))\n",
    "print(\"Label used: \", LABEL_COL)\n",
    "print(\"Features used: \", FEATURES_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability:  {0: -0.5680907948876823, 1: -0.8361084357191146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Prior probability\n",
    "def mapper(rows):\n",
    "    result = []\n",
    "    for row in rows:\n",
    "        result.append((row[LABEL_COL], 1))\n",
    "    return result\n",
    "\n",
    "def reducer(row):\n",
    "    key, iterable = row\n",
    "    count = 0\n",
    "    for val in iterable:\n",
    "        count += val\n",
    "    return [(key, count)]\n",
    "\n",
    "prior = df.rdd.mapPartitions(mapper).groupByKey().flatMap(reducer).collect()\n",
    "total = sum([x[1] for x in prior])\n",
    "prior_dict = dict([(x[0], log(x[1]/total)) for x in prior])\n",
    "print(\"Prior probability: \", prior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probability:  {0: {'Leg room service': {3: -1.3927378916221316, 5: -1.822360775947274, 4: -1.5891500255153403, 2: -1.4245131401365037, 1: -1.965696762032647, 0: -5.263116195778559}, 'Checkin service': {4: -1.3218538054942783, 1: -1.7924582157734041, 3: -1.322620823748565, 5: -1.9899515675430093, 2: -1.810000714960011, 0: -10.98014389718478}, 'Seat comfort': {5: -1.8493880045514404, 1: -1.8379753053119319, 2: -1.6286512454527142, 3: -1.3847452268780962, 4: -1.4399963077061697, 0: -10.98014389718478}, 'Inflight entertainment': {5: -1.8933280114940956, 1: -1.7020514231577792, 2: -1.4456207817201223, 4: -1.640443319902891, 3: -1.4380567215947164, 0: -8.341086567569523}, 'Flight Distance_bucket': {2: -2.1355192137994794, 0: -2.1936872188407803, 3: -2.117235602066153, 6: -2.203976797453506, 5: -2.120922503576649, 1: -2.106955993134675, 4: -2.1360959982422902, 8: -2.701969606241043, 9: -3.203189493862339, 7: -2.627825348958777}, 'Type of Travel_index': {1: -0.7097241862572721, 0: -0.676840496939187}, 'Departure/Arrival time convenient': {4: -1.325438268142877, 2: -1.8207810799734654, 5: -1.5201348949550333, 3: -1.7642191469146973, 1: -2.001357343882778, 0: -3.0520982963100036}, 'Age_bucket': {0: -1.989826949186564, 2: -2.1087788920479293, 6: -2.712694938879932, 5: -2.667517871509818, 1: -2.2281948391261674, 8: -2.508366569299026, 4: -2.3410871180117026, 9: -2.0150648817764267, 3: -2.2059856059788228, 7: -2.5599022318449935}, 'Online boarding': {3: -1.1390638047772697, 2: -1.3356213930029395, 5: -3.0985839801278816, 1: -1.854599176913314, 4: -1.6246647595423593, 0: -4.001930154554082}, 'Gate location': {1: -1.8937807050283089, 3: -1.149227037483488, 5: -2.2938832646530045, 2: -1.7260737623472728, 4: -1.3713630320373982}, 'Cleanliness': {5: -1.8918581712159037, 1: -1.7050462779933195, 2: -1.534494276995517, 4: -1.5397240828932757, 3: -1.4387028323892959, 0: -8.49523724739678}, 'Ease of Online booking': {3: -1.2475632966273533, 5: -2.7867436652326827, 2: -1.2585979736243218, 4: -1.8584161835652013, 1: -1.6839010962078944, 0: -3.6669235100944793}, 'Class_index': {2: -2.3439239993469023, 0: -1.3548502392281252, 1: -0.4368571995906317}, 'Inflight service': {5: -1.7168316404424921, 4: -1.08852651721914, 1: -2.4595564727005277, 3: -1.338410795090518, 2: -1.9960769695317353, 0: -9.88153160851667}, 'Inflight wifi service': {3: -1.1108855361993129, 2: -1.1099029384422883, 1: -1.5878987219109857, 4: -2.0036289248696684, 5: -6.316704803072714, 0: -8.900702355504945}, 'Arrival Delay in Minutes_bucket': {2: -2.2236188942578092, 1: -2.1496008865681846, 0: -0.6122307437169788, 3: -2.1420272995369602, 4: -2.1567904120709893}, 'Baggage handling': {4: -1.1099546300501673, 3: -1.3175184464565428, 1: -2.4480620932747925, 5: -1.7236835895950449, 2: -1.9822544469840644}, 'Departure Delay in Minutes_bucket': {3: -2.1927709084529043, 0: -0.5650909945476622, 2: -2.252851867975142, 4: -2.1773207374428933, 1: -2.286311932110087}, 'Customer Type_index': {0: -0.2825558458741199, 1: -1.401832621528014}, 'On-board service': {4: -1.4159124526464817, 1: -1.8167903164329349, 2: -1.6845436812577745, 3: -1.3262080417391662, 5: -1.9401174628782754, 0: -9.88153160851667}, 'Gender_index': {1: -0.7193316118684231, 0: -0.6676309153457309}, 'Food and drink': {5: -1.7663096359543422, 1: -1.74638001026501, 2: -1.4766116595832195, 4: -1.6270027265769267, 3: -1.4751210131120724, 0: -6.954792206449631}}, 1: {'Leg room service': {3: -2.106555954319698, 4: -0.9875261638768926, 5: -1.0889480219395882, 2: -2.1282094328942027, 1: -3.067685494796782, 0: -5.600138467996805}, 'Checkin service': {4: -1.214804057039778, 3: -1.2614312963326013, 2: -2.6263315550717796, 5: -1.2760856042812658, 1: -2.6807409257278136}, 'Seat comfort': {5: -0.9619064402204378, 4: -0.9265780016819068, 1: -2.8148297837574634, 3: -2.4321755406308223, 2: -2.601398673378859}, 'Inflight entertainment': {5: -1.012776513892071, 3: -2.166151263511659, 4: -0.9165580460367144, 1: -3.2504858641447734, 2: -2.482615137388892}, 'Flight Distance_bucket': {6: -2.444163951014639, 0: -2.526218774871024, 7: -1.9964101288051481, 5: -2.5794197666600835, 8: -1.9415319264622042, 2: -2.609842631873275, 1: -2.5694810704105535, 4: -2.596605374806579, 9: -1.7274325659094938, 3: -2.5805955457490954}, 'Type of Travel_index': {0: -0.07545383622574449, 1: -2.6217239597600277}, 'Departure/Arrival time convenient': {2: -1.7711042728119886, 3: -1.7442396842258752, 4: -1.5139596853088744, 0: -2.881700638533017, 1: -1.7869378269753213, 5: -1.553395018110396}, 'Age_bucket': {2: -2.4818154572183277, 9: -2.5237149482743177, 7: -1.9782100814258239, 3: -2.363115058177346, 5: -2.062502277756621, 6: -2.1116954663670553, 1: -2.5088222295580658, 4: -2.356511260593166, 8: -1.9630280079543283, 0: -3.241902120453382}, 'Online boarding': {5: -0.9137770219405249, 4: -0.854211549111977, 2: -3.1052417251337183, 1: -3.418428535751911, 0: -3.5064910799429843, 3: -2.7212107932620233}, 'Gate location': {2: -1.6128291831704902, 3: -1.5129479490833575, 4: -1.5575100331495273, 1: -1.6436951305598586, 5: -1.7346010553881064, 0: -10.712126256353349}, 'Cleanliness': {5: -1.182023918461973, 3: -1.4453110374491864, 4: -1.1293261778829855, 1: -2.8461708424198466, 2: -2.572393976635678}, 'Ease of Online booking': {2: -1.8180046149711353, 3: -1.7878692354652495, 4: -1.46680491548192, 5: -1.4821793547382471, 1: -1.9220093634608753, 0: -2.7147994333552505}, 'Class_index': {0: -0.2666051529270364, 1: -1.6443868529761878, 2: -3.1967816851729127}, 'Inflight service': {4: -0.9094540090560365, 3: -2.2289035845082643, 5: -1.001192431177424, 2: -2.569190245921082, 1: -3.0884843098417765}, 'Inflight wifi service': {2: -1.9466677248491213, 3: -1.9384321099689046, 4: -1.3344087749690334, 5: -1.3769169023312964, 1: -2.048929770817268, 0: -2.6768473452086816}, 'Arrival Delay in Minutes_bucket': {0: -0.45278409626239036, 4: -2.4783575471362504, 2: -2.530685560633975, 1: -2.1495773632163138, 3: -2.4754409336408916}, 'Baggage handling': {4: -0.9179514637579209, 5: -0.9964754412992851, 2: -2.5814782881927645, 3: -2.225804728604198, 1: -3.039833800724593}, 'Departure Delay in Minutes_bucket': {0: -0.46742771284883394, 1: -2.2477011304757664, 4: -2.4936086788637577, 3: -2.4555189117271907, 2: -2.3094464517258717}, 'Customer Type_index': {0: -0.1052194619866707, 1: -2.3038554721612976}, 'On-board service': {4: -0.999375570212522, 3: -1.8320971388849057, 5: -1.086370445415833, 1: -2.9753825738998527, 2: -2.4847507495193133}, 'Gender_index': {0: -0.6922343959471178, 1: -0.6940607991097851}, 'Food and drink': {5: -1.302607210516788, 4: -1.258134072431605, 3: -1.6283700364866185, 1: -2.863192529989277, 2: -1.6630115328763873, 0: -6.820305958242722}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Posterior probability\n",
    "def mapper(split):\n",
    "    result = []\n",
    "    dict = {}\n",
    "\n",
    "    for row in split:\n",
    "        for feature in FEATURES_COL:\n",
    "            if row[LABEL_COL] not in dict:\n",
    "                dict[row[LABEL_COL]] = {}\n",
    "            if feature not in dict[row[LABEL_COL]]:\n",
    "                dict[row[LABEL_COL]][feature] = {}\n",
    "            if row[feature] not in dict[row[LABEL_COL]][feature]:\n",
    "                dict[row[LABEL_COL]][feature][row[feature]] = 0\n",
    "            dict[row[LABEL_COL]][feature][row[feature]] += 1\n",
    "\n",
    "    for label in dict:\n",
    "        for feature in dict[label]:\n",
    "            for value in dict[label][feature]:\n",
    "                result.append((label, (feature, value, dict[label][feature][value])))\n",
    "    return result\n",
    "\n",
    "def reducer(sorted_pairs):\n",
    "    key, iterable = sorted_pairs\n",
    "    dict = {}\n",
    "    iterable = list(iterable)\n",
    "    total_count = 0\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(iterable)):\n",
    "        feature = iterable[i][0]\n",
    "        value = iterable[i][1]\n",
    "        value_count = iterable[i][2]\n",
    "        total_count += value_count\n",
    "        if feature not in dict:\n",
    "            dict[feature] = {}\n",
    "        if value not in dict[feature]:\n",
    "            dict[feature][value] = 0\n",
    "        dict[feature][value] += value_count\n",
    "\n",
    "    for feature in dict:\n",
    "        for value in dict[feature]:\n",
    "            result.append((key, feature, value, dict[feature][value] / total_count * len(FEATURES_COL)))\n",
    "    return result\n",
    "\n",
    "posterior = df.rdd.mapPartitions(mapper).groupByKey().flatMap(reducer).collect()\n",
    "posterior_dict={}\n",
    "for i in range(len(posterior)):\n",
    "    label = posterior[i][0]\n",
    "    feature = posterior[i][1]\n",
    "    value = posterior[i][2]\n",
    "    p_bin = posterior[i][3]\n",
    "    if label not in posterior_dict:\n",
    "        posterior_dict[label] = {}\n",
    "    if feature not in posterior_dict[label]:\n",
    "        posterior_dict[label][feature] = {}\n",
    "    posterior_dict[label][feature][value] = log(p_bin)\n",
    "\n",
    "print(\"Posterior probability: \", posterior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    dict = {}# Dictionary of label and probability\n",
    "    for label in prior_dict:\n",
    "        prob = prior_dict[label]\n",
    "        for feature in FEATURES_COL:\n",
    "            if row[feature] in posterior_dict[label][feature]:\n",
    "                prob += posterior_dict[label][feature][row[feature]]\n",
    "            else:\n",
    "                prob += log(0.000001)# Smoothing\n",
    "        dict[label] = prob\n",
    "    return max(dict, key=dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.894134795451474\n"
     ]
    }
   ],
   "source": [
    "# Trainig accuracy\n",
    "def mapper(rows):\n",
    "    result = []\n",
    "    for row in rows:\n",
    "        if predict(row) == row[LABEL_COL]:\n",
    "            result.append((0,1))\n",
    "        else:\n",
    "            result.append((0,0))\n",
    "    return result\n",
    "  \n",
    "def reducer(row):\n",
    "    count = 0\n",
    "    for val in row[1]:\n",
    "        count += val\n",
    "    return [(0, count)]\n",
    "\n",
    "correct = df.rdd.mapPartitions(mapper).groupByKey().flatMap(reducer).collect()[0][1]\n",
    "total = df.count()\n",
    "print(\"Training accuracy: \", correct / total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"../../DataFiles/airline-val.csv\")\n",
    "df_val,_,_ = process_dataframe(df_val, string_indexers, bucketizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8908585331942996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# validation accuracy\n",
    "correct = df_val.rdd.mapPartitions(mapper).groupByKey().flatMap(reducer).collect()[0][1]\n",
    "total = df_val.count()\n",
    "print(\"Validation accuracy: \", correct / total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of context analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bin_class1 [0.10821677 0.11653066 0.54214014 0.11741656 0.11569586]\n",
      "p_bin_class2 [0.6358554  0.08388088 0.07960443 0.1165334  0.08412589]\n",
      "p_class1_bin [0.18200573 0.64491797 0.89902814 0.56845925 0.6426003 ]\n",
      "p_class2_bin [0.81799427 0.35508203 0.10097186 0.43154075 0.3573997 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xb1d\\x07\\ty\\x92`\\xee\\xb7\\xbc\\x13\\xfa\\x16\\xa2}\\x089d F\\xb5\\x92QTu\\xe5-\\xaeq\\x86ryC\\xa8=\\x18-\\xe3\\xdd\\x0f\\xd4i\\x01\\x81\\xcd\\xe9\\x91\\xd1\\xda)\\x02\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t', b'7.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b\"\\xe3(\\xb4\\x85z\\xfa\\x87\\ta\\xd9\\x08\\xc8\\x04\\xa3\\x90S\\xbb+\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\"]\n",
      "Bad pipe message: %s [b'X\\xcfi\\xd49\\xd9\\xad\\xa90c\\xeeC\\xba\\x8d\\xdcSCb\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17']\n",
      "Bad pipe message: %s [b'O\\xe8p\\xdaj\\x0c9\\x97\\xa3K\\x94\\x8c;\\xaa\\xc8\\xa2\\xe1\\x8c\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15']\n",
      "Bad pipe message: %s [b't\\xc7@6\\xfe;\\x10G!V\\x83@\\xba\\xdd,\\xa8\\xffc\\x00\\x00\\xa2\\xc0\\x14']\n",
      "Bad pipe message: %s [b'\\x90\\xf9\\xa2>?k\\x8d1\\xd07\\xa0|}D\\xfe\\x05\\x1eH\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00']\n",
      "Bad pipe message: %s [b\"\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03']\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 557, in loads\n",
      "    length = read_int(stream)\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 0: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 0: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 8: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb7 in position 7: invalid start byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 0: invalid start byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc7 in position 7: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 0: invalid start byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 9: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdc in position 7: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 0: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 7: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 62, in worker\n",
      "  File \"/usr/local/python/3.10.4/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 563, in loads\n",
      "    return s.decode(\"utf-8\") if self.use_unicode else s\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_bin_class1 = np.array(list(posterior_dict[0]['Arrival Delay in Minutes_bucket'].values()))\n",
    "p_bin_class1 = np.exp(p_bin_class1)\n",
    "print('p_bin_class1',p_bin_class1)\n",
    "p_bin_class2 = np.array(list(posterior_dict[1]['Arrival Delay in Minutes_bucket'].values()))\n",
    "p_bin_class2 = np.exp(p_bin_class2)\n",
    "print('p_bin_class2',p_bin_class2)\n",
    "p_class1 = np.exp(prior_dict[0])\n",
    "p_class2 = np.exp(prior_dict[1])\n",
    "p_bin = p_class1 * p_bin_class1+ p_class2 * p_bin_class2\n",
    "p_class1_bin = p_class1 * p_bin_class1 / p_bin\n",
    "p_class2_bin = p_class2 * p_bin_class2 / p_bin\n",
    "print('p_class1_bin',p_class1_bin)\n",
    "print('p_class2_bin',p_class2_bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
